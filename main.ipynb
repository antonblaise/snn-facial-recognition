{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Facial Recognition with Siamese Neural Network*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import standard dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Tensorflow dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set GPU growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Avoid out-of-memory error by limiting GPU comsumption\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# for gpu in gpus:\n",
    "#     print(gpu)\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create folder structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "\n",
    "POS_PATH = os.path.join('data', 'positive') # ./data/positive\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create those folders\n",
    "\n",
    "if not os.path.exists(POS_PATH):\n",
    "    os.makedirs(POS_PATH)\n",
    "if not os.path.exists(NEG_PATH):\n",
    "    os.makedirs(NEG_PATH)\n",
    "if not os.path.exists(ANC_PATH):\n",
    "    os.makedirs(ANC_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect positives and anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Untar labelled faces in the wild dataset\n",
    "### http://vis-www.cs.umass.edu/lfw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put all those images into negative folder, as they are all negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move FaceID files into the following directory: data/negative\n",
    "\n",
    "for directory in os.listdir('FaceID'):\n",
    "    for file in os.listdir(os.path.join('FaceID', directory)):\n",
    "        OLD_PATH = os.path.join('FaceID', directory, file)\n",
    "        NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "        os.replace(OLD_PATH, NEW_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect positive and anchor classes\n",
    "* Anchor = input\n",
    "* Positive = Correct\n",
    "* Negative = Wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import uuid library to generate unique image names\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access webcam\n",
    "\n",
    "cap = cv2.VideoCapture(2)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read() # ret = return value; frame = the actual image captured on webcam\n",
    "\n",
    "    # slice/reshape our frame to size 250x250\n",
    "    frame = frame[130:250+130, 200:200+250, :]\n",
    "\n",
    "    # collect anchors when hit 'A'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('a'):\n",
    "        imgName = os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1())) # image name and full path to save\n",
    "        cv2.imwrite(imgName, frame) # save image\n",
    "\n",
    "    # Collect positives when hit 'P'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('p'):\n",
    "        imgName = os.path.join(POS_PATH, '{}.jpg'.format(uuid.uuid1())) # image name and full path to save\n",
    "        cv2.imwrite(imgName, frame) # save image\n",
    "\n",
    "    cv2.imshow('Image Collection', frame) # render/show the captured image onto the screen\n",
    "\n",
    "    # quit when hit 'Q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# disconnect webcam, close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get image directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 300 samples/images from each (must have matching number of samples)\n",
    "\n",
    "anchor = tf.data.Dataset.list_files(f'{ANC_PATH}\\*.jpg').take(300)\n",
    "negative = tf.data.Dataset.list_files(f'{NEG_PATH}\\*.jpg').take(300)\n",
    "positive = tf.data.Dataset.list_files(f'{POS_PATH}\\*.jpg').take(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show what's contained inside those three variables\n",
    "# showAnchor = anchor.as_numpy_iterator()\n",
    "# showAnchor.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing - Scale and Resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read JPEG picture file as bytes\n",
    "2. Decode the bytes in as JPEG\n",
    "3. Resize the image/data into 100x100\n",
    "4. Scale the data into between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    byte_img = tf.io.read_file(file_path) # read the file's data as bytes\n",
    "    img = tf.io.decode_jpeg(byte_img) # decode the bytes as JPEG and store into img variable\n",
    "    img = tf.image.resize(img, (100,100)) # resize the img into 100x100\n",
    "    img /= 255.0 # Without this step the image will be super bright. Try it.\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the 'preprocess' function, and show the resultant image\n",
    "# img = preprocess('data\\\\anchor\\\\35fe2f84-a2fd-11ec-922d-ace2d36277c6.jpg')\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create labelled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (anchor, positive) => 1,1,1,1,1\n",
    "# (anchor, negative) => 0,0,0,0,0\n",
    "\n",
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor))))) # All 1's, as anchor matches the positive\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor))))) # All 0's, as anchor matches the negative\n",
    "data = positives.concatenate(negatives) # joining positive and negative together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The tf.data.zip() function is used for creating a dataset by zipping together a dict, array, or nested structure of Dataset.\n",
    "* All these three variables are tuples with 3 elements, namely (input image, comparison image, label - 1/0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show what's contained inside those three variables\n",
    "# Positives\n",
    "# showPositives = positives.as_numpy_iterator()\n",
    "# showPositives.next()\n",
    "\n",
    "# Negatives\n",
    "# showNegatives = negatives.as_numpy_iterator()\n",
    "# showNegatives.next()\n",
    "\n",
    "# Positives\n",
    "showData = data.as_numpy_iterator()\n",
    "example = showData.next()\n",
    "showData.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build train-test partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    return (preprocess(input_img), preprocess(validation_img), label)\n",
    "\n",
    "# Sample input tuple for the function: \n",
    "# (b'data\\\\anchor\\\\3c539b2a-a2fd-11ec-9a85-ace2d36277c6.jpg',   <- Innput\n",
    "#  b'data\\\\positive\\\\d56b4449-a2fd-11ec-b5f5-ace2d36277c6.jpg', <- Comparison\n",
    "#  1.0)                                                         <- Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = preprocess_twin(*example) # * = unpack the tuple, so we dont have to type each input arguments one by one ourselves\n",
    "\n",
    "# result = (preprocessed input image, preprocessed comparison image, label - 1 as correct, 0 as wrong)\n",
    "print(type(result))\n",
    "\n",
    "f, axarr = plt.subplots(2,1)\n",
    "axarr[0].imshow(result[0])\n",
    "axarr[1].imshow(result[1])\n",
    "print(result[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataloader pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. pass all the stuff in 'data' into the 'preprocess_twin' function conveniently using the '.map' method.\n",
    "2. store the consequent outputs in cache\n",
    "3. shuffle the positives and negatives for effective training\n",
    "4. done preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=1024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now the length of 'data' is 600, as it contains both positives and negatives, each at 300. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display training data individually/one by one\n",
    "\n",
    "print(type(data))\n",
    "\n",
    "ddd = data.as_numpy_iterator()\n",
    "print(type(ddd))\n",
    "\n",
    "dddd = ddd.next()\n",
    "\n",
    "f, axarr = plt.subplots(2,1)\n",
    "\n",
    "axarr[0].imshow(dddd[0])\n",
    "axarr[1].imshow(dddd[1])\n",
    "print(dddd[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python's map() is a built-in function that allows you to process and transform all the items in an iterable without using an explicit for loop, a technique commonly known as mapping. map() is useful when you need to apply a transformation function to each item in an iterable and transform them into a new iterable.\n",
    "* Function caching allows us to cache the return values of a function depending on the arguments. It can save time when an I/O bound function is periodically called with the same arguments. Before Python 3.2 we had to write a custom implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data`</br>\n",
    "<ShuffleDataset element_spec=(TensorSpec(shape=(100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training partition\n",
    "\n",
    "train_data = data.take(round(len(data) * 0.7)) # Round off the 70% of the length of 'data'\n",
    "train_data = train_data.batch(16) # Make batches of 16\n",
    "train_data = train_data.prefetch(8) # Start preprocessing the next set of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_data`</br>\n",
    "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(None, 100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One extra dimension is created due to the _batch()_ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing partition\n",
    "\n",
    "test_data = data.skip(round(len(data) * 0.7)) # Avoid taking the train_data\n",
    "test_data = test_data.take(round(len(data) * 0.3)) # Take the rest 30% of 'data'\n",
    "test_data = test_data.batch(16) # Make batches of 16\n",
    "test_data = test_data.prefetch(8) # Start preprocessing the next set of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding():\n",
    "    inp = Input(shape=(100,100,3), name='input_image')\n",
    "\n",
    "    # 1st block\n",
    "    c1 = Conv2D(64, (10,10), activation='relu')(inp)\n",
    "    m1 = MaxPooling2D(64, (2,2), padding='same')(c1)\n",
    "\n",
    "    # 2nd block\n",
    "    c2 = Conv2D(128, (7,7), activation='relu')(m1)\n",
    "    m2 = MaxPooling2D(64, (2,2), padding='same')(c2)\n",
    "    \n",
    "    # 3rd block\n",
    "    c3 = Conv2D(128, (4,4), activation='relu')(m2)\n",
    "    m3 = MaxPooling2D(64, (2,2), padding='same')(c3)\n",
    "\n",
    "    # Final embedding block\n",
    "    c4 = Conv2D(256, (4,4), activation='relu')(m3)\n",
    "    f1 = Flatten()(c4)\n",
    "    d1 = Dense(4096, activation='sigmoid')(f1)\n",
    "\n",
    "    return Model(inputs=[inp], outputs=[d1], name='embedding') # Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = make_embedding()\n",
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Embedding = the Siamese Neural Network structure up until the Feature Vector 4096 without the L1 siamese dist.\n",
    "* Its input = image data with dimension 100x100x3\n",
    "* Its output = vector with length 4096\n",
    "* This embedding is just a structure for data to be passed through and processed. No data has been given to it yet.\n",
    "</br></br>*refer Model.png*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build distance layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Dist(Layer): # from tensorflow.keras.layers import Layer\n",
    "\n",
    "    # init method - inheritance\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    \n",
    "    # Combine the two rivers\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So now this is the L1 siamese dist part in the pipeline.\n",
    "</br></br>\n",
    "*refer Model.png*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Siamese model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. input/validation image = a data/tensor with dimension 100x100x3\n",
    "2. inp/val embedding = a flat data/tensor with length 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*\\*\\*Everything from this point on until the 'def make_siamese_model():' is for the purpose of explaining what does the function do. </br>Please comment them out unless you want to run them one by one and check the outputs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the image as input data\n",
    "input_image = Input(name='input_img', shape=(100, 100, 3))\n",
    "validation_image = Input(name='validation_img', shape=(100, 100, 3))\n",
    "\n",
    "# Pass those image data into the embedding that we constructed before\n",
    "inp_embedding = embedding(input_image)\n",
    "val_embedding = embedding(validation_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`input_image` & `validation_image`\n",
    "</br></br>\n",
    "<KerasTensor: shape=(None, 100, 100, 3) dtype=float32 (created by layer 'input_img')></br>\n",
    "<KerasTensor: shape=(None, 100, 100, 3) dtype=float32 (created by layer 'validation_img')>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*- After passing through the embedding (pipeline):*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`inp_embedding` & `val_embedding`\n",
    "</br></br>\n",
    "<KerasTensor: shape=(None, 4096) dtype=float32 (created by layer 'embedding')></br>\n",
    "<KerasTensor: shape=(None, 4096) dtype=float32 (created by layer 'embedding')>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*refer Model.png*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_layer = L1Dist()\n",
    "distances = siamese_layer(inp_embedding, val_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Siamese layer = input embedding - validation embedding\n",
    "* A minus between 2 vectors of length 4096 is performed to obtain the absolute difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`distances`\n",
    "</br>\n",
    "<KerasTensor: shape=(None, 4096) dtype=float32 (created by layer 'l1_dist_1')>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The siamese layer is where the comparison between the input and validation images happens.\n",
    "* The class 'L1Dist' is passed into a variable/placehoder named 'siamese_layer'\n",
    "* And then 'siamese_layer' takes in 2 inputs, namely the input embedding and validation embedding, which are the two rivers, 2 streams of neural network pipelines that process the input image (anchor) and the validation image (positive/negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Dense(1, activation='sigmoid')(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`classifier`\n",
    "</br>\n",
    "<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'dense_5')>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After making the comparison, the result is passed into the final layer - the output, named 'classifier', which is has a shape of only 1x1, as the answer is just yes/no. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`siameseNetwork`</br>\n",
    "run it and you will see the full SNN model that is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siameseNetwork = Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')\n",
    "siameseNetwork.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_siamese_model():\n",
    "\n",
    "    # Anchor image input\n",
    "    input_image = Input(name='input_img', shape=(100, 100, 3))\n",
    "\n",
    "    # Validation image (comparison)\n",
    "    validation_image = Input(name='validation_img', shape=(100, 100, 3))\n",
    "\n",
    "    # Combine siamese distance components\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = 'distance'\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "\n",
    "    # Classification layer (check - are they similar?)\n",
    "    classifier = Dense(1, activation='sigmoid')(distances) # refer Model.png\n",
    "\n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier, name='SiameseNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_model = make_siamese_model()\n",
    "snn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recap:*\n",
    "</br> \n",
    "* Before this, we created the model up until the Feature vector and named it `'embedding'`.\n",
    "* Then we **create a new layer** called *`'siamese_layer'`* where the streams of input and validation meets and are compared.\n",
    "* The comparison result is stored in the variable 'distances'.\n",
    "* The comparison result in 'distances' is passed into a **new layer**, which is the *`output layer`*, and is given the name 'classifier'.\n",
    "* The output layer has dimension of one single value (1x1), because we want a YES/NO answer after all.\n",
    "* Now, the full siamese neural network model is complete, with 3 parts that we just joined:\n",
    "    - Embeddings: Convolution-ReLU-MaxPooling\n",
    "    - Siamese layer: for comparison, where minus is done\n",
    "    - Output layer: Single value output, yes/no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*refer Model.png*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup loss and optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss\n",
    "binary_cross_loss = tf.losses.BinaryCrossentropy()\n",
    "\n",
    "# Define optimiser\n",
    "opt = tf.keras.optimizers.Adam(1e-4) # learning rate = 0.0004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=snn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build train step function\n",
    "* train_step() function is focused on training for one batch.\n",
    "* Hence, a training loop is needed afterwards to iterate over every batch in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "\n",
    "    # Record all of our operations\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get anchor and positive/negative image\n",
    "        X = batch[:2]\n",
    "\n",
    "        # Get label\n",
    "        y = batch[2]\n",
    "\n",
    "        # Forward pass\n",
    "        yhat = snn_model(X, training=True)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = binary_cross_loss(y, yhat) # declared/defined under 'Setup loss and optimiser'\n",
    "\n",
    "    print(loss)\n",
    "\n",
    "    # Calculate gradients\n",
    "    grad = tape.gradient(loss, snn_model.trainable_variables)\n",
    "\n",
    "    # Calculate updated weights and apply to the siamese model\n",
    "    opt.apply_gradients(zip(grad, snn_model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* @tf.function = Compiles a function into a callable TensorFlow graph. (deprecated arguments)\n",
    "</br>https://www.tensorflow.org/api_docs/python/tf/function</br>\n",
    "* The test_data now consists 3 parts:\n",
    "    1. A batch of 16 pieces of anchor images\n",
    "    2. A batch of 16 pieces of positive/negative images\n",
    "    3. 16 labels of whether it is 1/0, meaning correct/wrong\n",
    "* The 'batch' input of the function is where the train_data is passed into.\n",
    "* Dimension/Shape of X = (2, 16, 100, 100, 3).\n",
    "    - 2: Anchor image & Positive/Negative image\n",
    "    - 16: Batch of 16 pieces\n",
    "    - (100, 100): Dimension/Size/Resolution of each image, namely 100x100\n",
    "    - 3: 3 color channels images, namely RGB.\n",
    "* y = actual labels\n",
    "* yhat = predicted y value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, EPOCHS): \n",
    "    # Loop through the epochs\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "\n",
    "        # Loop through each batch\n",
    "        for idx, batch in enumerate(data):\n",
    "            # Run train step here\n",
    "            train_step(batch)\n",
    "            progbar.update(idx+1)\n",
    "\n",
    "        # Save checkpoints\n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix) # Both 'checkpoint' and 'checkpoint_prefix' are defined under 'Establish checkpoints' above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* idx = index\n",
    "* For this 'train()' function, pass in 'train_data' as _data_, and number of epochs as _EPOCHS_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "train(train_data, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.load_checkpoint(\n",
    "    'training_checkpoints'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metric calculations\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "# Unpack the 3 components of test_data into individual parts\n",
    "test_input, test_val, y_true = test_data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* test_input = what we are going to grab form our webcam. Length = 16, as one batch has 16 members.\n",
    "* test_val = positive/negative. Length is also 16.\n",
    "* y_true = the correct labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = snn_model.predict([test_input, test_val])\n",
    "\n",
    "# Post-process the results\n",
    "[1 if prediction >= 0.5 else 0 for prediction in y_hat ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A simple, non-numpy array is made containing the 16 prediction results from the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a metric object\n",
    "m = Recall()\n",
    "\n",
    "# Calculating the recall value\n",
    "m.update_state(y_true, y_hat)\n",
    "\n",
    "# Return Recall Result \n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1.0 means 100% accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a metric object\n",
    "m = Precision()\n",
    "\n",
    "# Calculating the recall value\n",
    "m.update_state(y_true, y_hat)\n",
    "\n",
    "# Return Recall Result \n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(test_input[2]) # can take from index 0-15\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(test_val[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_model.save('snn_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_model.save('snn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('snn_model.h5', custom_objects={'L1Dist':L1Dist, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('snn_model', custom_objects={'L1Dist':L1Dist, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(model.predict([test_input, test_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(model, detection_threshold, verification_threshold):\n",
    "    results = []\n",
    "    for image in os.listdir(os.path.join('application_data','verification_images')): # loop through every image in the 'verification_images' folder\n",
    "        input_img = preprocess(os.path.join('application_data', 'input_image', 'input_image.jpg')) # snap one image from the webcam as save it as 'input_image.jpg' inside that directory\n",
    "        verification_img = preprocess(os.path.join('application_data', 'verification_images', image))\n",
    "        results.append(model.predict(list(np.expand_dims([input_img, verification_img], axis=1)))) # compare the input image with the verification images\n",
    "        \n",
    "    # Sum up all the results that exceeds the detection threshold\n",
    "    detection = np.sum(np.array(results) > detection_threshold)\n",
    "\n",
    "    # Proportion of verification\n",
    "    verification = detection / len(os.listdir(os.path.join('application_data', 'verification_images')))\n",
    "    \n",
    "    # if proportion of verification > verification threshold, then verified = True\n",
    "    verified = verification > verification_threshold\n",
    "\n",
    "    return results, verified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* frame = input image\n",
    "* model = the SNN model\n",
    "* detection_threshold = metric in which a prediction is considered positive\n",
    "* verification_threshold = the proportion of ( positive predictions / total positive samples )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV real-time verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import messagebox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(2)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    frame = frame[130:250+130, 200:200+250, :]\n",
    "\n",
    "    cv2.imshow('Verification', frame)\n",
    "\n",
    "    # Verification trigger\n",
    "    if cv2.waitKey(10) & 0xFF == ord('v'):\n",
    "        # save the input image into the 'application_data\\input_image' folder\n",
    "        cv2.imwrite(os.path.join('application_data', 'input_image', 'input_image.jpg'), frame)\n",
    "\n",
    "        # verification function\n",
    "        results, verified = verify(model, 0.7, 0.7)\n",
    "        if verified:\n",
    "            messagebox.showinfo('Message', 'verification SUCCESS!')\n",
    "            # print('verification SUCCESS!')\n",
    "        else:\n",
    "            messagebox.showinfo('Message', 'verification FAILED')\n",
    "            # print('verification FAILED')\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1dba1be69361d3341fbbf95a8e1c71f4b3704a2dc9b9f4ab36237200da35abfa"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
